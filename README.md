
## 00_Organizations

#### 0_OpenAI
* [2017 (OpenAI) (NIPS) [RLHF] Deep Reinforcement Learning from Human Preferences](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2017%20%28OpenAI%29%20%28NIPS%29%20%5BRLHF%5D%20Deep%20Reinforcement%20Learning%20from%20Human%20Preferences.pdf) <br />
* [2018 (OpenAI) (Arxiv) [GPT-1] Improving Language Understanding by Generative Pre-Training](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2018%20%28OpenAI%29%20%28Arxiv%29%20%5BGPT-1%5D%20Improving%20Language%20Understanding%20by%20Generative%20Pre-Training.pdf) <br />
* [2019 (OpenAI) (Arxiv) [GPT-2] Language Models are Unsupervised Multitask Learners](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2019%20%28OpenAI%29%20%28Arxiv%29%20%5BGPT-2%5D%20Language%20Models%20are%20Unsupervised%20Multitask%20Learners.pdf) <br />
* [2020 (OpenAI) (Arxiv) [GPT-3] Language Models are Few-Shot Learners](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2020%20%28OpenAI%29%20%28Arxiv%29%20%5BGPT-3%5D%20Language%20Models%20are%20Few-Shot%20Learners.pdf) <br />
* [2020 (OpenAI) (Arxiv) [Scaling laws] Scaling Laws for Neural Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2020%20%28OpenAI%29%20%28Arxiv%29%20%5BScaling%20laws%5D%20Scaling%20Laws%20for%20Neural%20Language%20Models.pdf) <br />
* [2021 (OpenAI) (Arxiv) Evaluating Large Language Models Trained on Code](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2021%20%28OpenAI%29%20%28Arxiv%29%20Evaluating%20Large%20Language%20Models%20Trained%20on%20Code.pdf) <br />
* [2022 (OpenAI) (Arxiv) WebGPT - Browser-assisted question-answering with human feedback](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2022%20%28OpenAI%29%20%28Arxiv%29%20WebGPT%20-%20Browser-assisted%20question-answering%20with%20human%20feedback.pdf) <br />
* [2022 (OpenAI) (Arxiv) [InstructGPT] [RLHF] Training language models to follow instructions with human feedback](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2022%20%28OpenAI%29%20%28Arxiv%29%20%5BInstructGPT%5D%20%5BRLHF%5D%20Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.pdf) <br />
* [2022 (OpenAI) (Arxiv) [WebGPT] Learning to summarize from human feedback](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2022%20%28OpenAI%29%20%28Arxiv%29%20%5BWebGPT%5D%20Learning%20to%20summarize%20from%20human%20feedback.pdf) <br />
* [2022 (OpenAI) (Arxiv) [WebGPT] WebGPT - Browser-assisted question-answering with human feedback](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2022%20%28OpenAI%29%20%28Arxiv%29%20%5BWebGPT%5D%20WebGPT%20-%20Browser-assisted%20question-answering%20with%20human%20feedback.pdf) <br />
* [2023 (OpenAI) (Arxiv) [GPT4] GPT-4 Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2023%20%28OpenAI%29%20%28Arxiv%29%20%5BGPT4%5D%20GPT-4%20Technical%20Report.pdf) <br />
* [2023 (OpenAI) GPT-4V(ision) System Card](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/0_OpenAI/2023%20%28OpenAI%29%20GPT-4V%28ision%29%20System%20Card.pdf) <br />

#### 1_Google
* [2013 (Google) (NIPS) [Word2vec] Distributed Representations of Words and Phrases and their Compositionality](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2013%20%28Google%29%20%28NIPS%29%20%5BWord2vec%5D%20Distributed%20Representations%20of%20Words%20and%20Phrases%20and%20their%20Compositionality.pdf) <br />
* [2014 (Google) (NIPS) [Seq2Seq] Sequence to Sequence Learning with Neural Networks](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2014%20%28Google%29%20%28NIPS%29%20%5BSeq2Seq%5D%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf) <br />
* [2017 (Google) (NIPS) [Transformer] Attention Is All You Need](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2017%20%28Google%29%20%28NIPS%29%20%5BTransformer%5D%20Attention%20Is%20All%20You%20Need.pdf) <br />
* [2019 (Google) (NAACL) [Bert] BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2019%20%28Google%29%20%28NAACL%29%20%5BBert%5D%20BERT%20-%20Pre-training%20of%20Deep%20Bidirectional%20Transformers%20for%20Language%20Understanding.pdf) <br />
* [2020 (Google) (ICLR) [ALBERT] ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2020%20%28Google%29%20%28ICLR%29%20%5BALBERT%5D%20ALBERT%20-%20A%20Lite%20BERT%20for%20Self-supervised%20Learning%20of%20Language%20Representations.pdf) <br />
* [2020 (Google) (JMLR) [T5] Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2020%20%28Google%29%20%28JMLR%29%20%5BT5%5D%20Exploring%20the%20Limits%20of%20Transfer%20Learning%20with%20a%20Unified%20Text-to-Text%20Transformer.pdf) <br />
* [2021 (Google) (ICLR) [VIT] An Image is Worth 16x16 Words - Transformers for Image Recognition at Scale](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2021%20%28Google%29%20%28ICLR%29%20%5BVIT%5D%20An%20Image%20is%20Worth%2016x16%20Words%20-%20Transformers%20for%20Image%20Recognition%20at%20Scale.pdf) <br />
* [2022 (Google) (Arxiv) [PaLM] PaLM - Scaling Language Modeling with Pathways](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2022%20%28Google%29%20%28Arxiv%29%20%5BPaLM%5D%20PaLM%20-%20Scaling%20Language%20Modeling%20with%20Pathways.pdf) <br />
* [2022 (Google) (JMLR) [SwitchTransfomers] Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2022%20%28Google%29%20%28JMLR%29%20%5BSwitchTransfomers%5D%20Switch%20Transformers%20-%20Scaling%20to%20Trillion%20Parameter%20Models%20with%20Simple%20and%20Efficient%20Sparsity.pdf) <br />
* [2022 (Google) (NIPS) [COT] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2022%20%28Google%29%20%28NIPS%29%20%5BCOT%5D%20Chain-of-Thought%20Prompting%20Elicits%20Reasoning%20in%20Large%20Language%20Models.pdf) <br />
* [2022 (Google) (NIPS) [ChainOfThought] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2022%20%28Google%29%20%28NIPS%29%20%5BChainOfThought%5D%20Chain-of-Thought%20Prompting%20Elicits%20Reasoning%20in%20Large%20Language%20Models.pdf) <br />
* [2022 (Google) (TMLR) [Emergent] Emergent Abilities of Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2022%20%28Google%29%20%28TMLR%29%20%5BEmergent%5D%20Emergent%20Abilities%20of%20Large%20Language%20Models.pdf) <br />
* [2023 (DeepMind) (NIPS) [TOT] Tree of Thoughts - Deliberate Problem Solving with Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2023%20%28DeepMind%29%20%28NIPS%29%20%5BTOT%5D%20Tree%20of%20Thoughts%20-%20Deliberate%20Problem%20Solving%20with%20Large%20Language%20Models.pdf) <br />
* [2023 (Google (Arxiv) [SIGLIP] Sigmoid Loss for Language Image Pre-Training](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2023%20%28Google%20%28Arxiv%29%20%5BSIGLIP%5D%20Sigmoid%20Loss%20for%20Language%20Image%20Pre-Training.pdf) <br />
* [2023 (Google) (ICLR) Self-Consistency Improves Chain of Thought Reasoning in Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2023%20%28Google%29%20%28ICLR%29%20Self-Consistency%20Improves%20Chain%20of%20Thought%20Reasoning%20in%20Language%20Models.pdf) <br />
* [2023 (Google) (ICLR) [ReAct] ReAct - Synergizing Reasoning and Acting in Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2023%20%28Google%29%20%28ICLR%29%20%5BReAct%5D%20ReAct%20-%20Synergizing%20Reasoning%20and%20Acting%20in%20Language%20Models.pdf) <br />
* [2023 (Google) (ICML) [PaLM-E] PaLM-E - An Embodied Multimodal Language Model](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2023%20%28Google%29%20%28ICML%29%20%5BPaLM-E%5D%20PaLM-E%20-%20An%20Embodied%20Multimodal%20Language%20Model.pdf) <br />
* [2024 (Google) (Arxiv) [Gemini 1.5] Gemini 1.5 - Unlocking multimodal understanding across millions of tokens of context](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2024%20%28Google%29%20%28Arxiv%29%20%5BGemini%201.5%5D%20Gemini%201.5%20-%20Unlocking%20multimodal%20understanding%20across%20millions%20of%20tokens%20of%20context.pdf) <br />
* [2024 (Google) (Arxiv) [Gemini] Gemini - A Family of Highly Capable Multimodal Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2024%20%28Google%29%20%28Arxiv%29%20%5BGemini%5D%20Gemini%20-%20A%20Family%20of%20Highly%20Capable%20Multimodal%20Models.pdf) <br />
* [2024 (Google) (Arxiv) [Gemma] Gemma - Open Models Based on Gemini Research and Technology](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2024%20%28Google%29%20%28Arxiv%29%20%5BGemma%5D%20Gemma%20-%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology.pdf) <br />
* [2024 (Google) (ICLR) [OPRO] Large Language Models as Optimizers](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2024%20%28Google%29%20%28ICLR%29%20%5BOPRO%5D%20Large%20Language%20Models%20as%20Optimizers.pdf) <br />
* [2025 (Google (Arxiv) [SIGLIP2] SigLIP 2 - Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2025%20%28Google%20%28Arxiv%29%20%5BSIGLIP2%5D%20SigLIP%202%20-%20Multilingual%20Vision-Language%20Encoders%20with%20Improved%20Semantic%20Understanding%2C%20Localization%2C%20and%20Dense%20Features.pdf) <br />
* [2025 (Google) (Arxiv) [Gemini 2.5] Gemini 2.5 - Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2025%20%28Google%29%20%28Arxiv%29%20%5BGemini%202.5%5D%20Gemini%202.5%20-%20Pushing%20the%20Frontier%20with%20Advanced%20Reasoning%2C%20Multimodality%2C%20Long%20Context%2C%20and%20Next%20Generation%20Agentic%20Capabilities.pdf) <br />
* [2025 (Google) (Arxiv) [Gemma3] Gemma 3 Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/1_Google/2025%20%28Google%29%20%28Arxiv%29%20%5BGemma3%5D%20Gemma%203%20Technical%20Report.pdf) <br />

#### 2_DeepSeek
* [2024 (DeepSeek) DeepSeek LLM Scaling Open-Source Language Models with Longtermism](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20DeepSeek%20LLM%20Scaling%20Open-Source%20Language%20Models%20with%20Longtermism.pdf) <br />
* [2024 (DeepSeek) [DeepSeek-Coder-V2] DeepSeek-Coder-V2 - Breaking the Barrier of Closed-Source Models in Code Intelligence](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20%5BDeepSeek-Coder-V2%5D%20DeepSeek-Coder-V2%20-%20Breaking%20the%20Barrier%20of%20Closed-Source%20Models%20in%20Code%20Intelligence.pdf) <br />
* [2024 (DeepSeek) [DeepSeek-Coder] DeepSeek-Coder - When the Large Language Model Meets Programming - The Rise of Code Intelligence](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20%5BDeepSeek-Coder%5D%20DeepSeek-Coder%20-%20When%20the%20Large%20Language%20Model%20Meets%20Programming%20-%20The%20Rise%20of%20Code%20Intelligence.pdf) <br />
* [2024 (DeepSeek) [DeepSeek-V2]  DeepSeek-V2 - A Strong, Economical, and Efficient Mixture-of-Experts Language Mode](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20%5BDeepSeek-V2%5D%20%20DeepSeek-V2%20-%20A%20Strong%2C%20Economical%2C%20and%20Efficient%20Mixture-of-Experts%20Language%20Mode.pdf) <br />
* [2024 (DeepSeek) [DeepSeek-VL] DeepSeek-VL - Towards Real-World Vision-Language Understanding](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20%5BDeepSeek-VL%5D%20DeepSeek-VL%20-%20Towards%20Real-World%20Vision-Language%20Understanding.pdf) <br />
* [2024 (DeepSeek) [DeepSeekMoE] DeepSeekMoE - Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20%5BDeepSeekMoE%5D%20DeepSeekMoE%20-%20Towards%20Ultimate%20Expert%20Specialization%20in%20Mixture-of-Experts%20Language%20Models.pdf) <br />
* [2024 (DeepSeek) [GRPO] DeepSeekMath - Pushing the Limits of Mathematical Reasoning in Open Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2024%20%28DeepSeek%29%20%5BGRPO%5D%20DeepSeekMath%20-%20Pushing%20the%20Limits%20of%20Mathematical%20Reasoning%20in%20Open%20Language%20Models.pdf) <br />
* [2025 (DeepSeek) (Nature) [DeepSeek-R1] DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2025%20%28DeepSeek%29%20%28Nature%29%20%5BDeepSeek-R1%5D%20DeepSeek-R1%20incentivizes%20reasoning%20in%20LLMs%20through%20reinforcement%20learning.pdf) <br />
* [2025 (DeepSeek) [DeepSeek-OCR] DeepSeek-OCR - Contexts Optical Compression](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2025%20%28DeepSeek%29%20%5BDeepSeek-OCR%5D%20DeepSeek-OCR%20-%20Contexts%20Optical%20Compression.pdf) <br />
* [2025 (DeepSeek) [DeepSeek-R1] DeepSeek-R1 -Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2025%20%28DeepSeek%29%20%5BDeepSeek-R1%5D%20DeepSeek-R1%20-Incentivizing%20Reasoning%20Capability%20in%20LLMs%20via%20Reinforcement%20Learning.pdf) <br />
* [2025 (DeepSeek) [DeepSeek-V3.2] DeepSeek-V3.2 - Pushing the Frontier of Open Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2025%20%28DeepSeek%29%20%5BDeepSeek-V3.2%5D%20DeepSeek-V3.2%20-%20Pushing%20the%20Frontier%20of%20Open%20Large%20Language%20Models.pdf) <br />
* [2025 (DeepSeek) [DeepSeek-V3] DeepSeek-V3 Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/2_DeepSeek/2025%20%28DeepSeek%29%20%5BDeepSeek-V3%5D%20DeepSeek-V3%20Technical%20Report.pdf) <br />

#### 3_Alibaba
* [2023 (Alibaba) (Arxiv) [QWEN] QWEN Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2023%20%28Alibaba%29%20%28Arxiv%29%20%5BQWEN%5D%20QWEN%20Technical%20Report.pdf) <br />
* [2023 (Alibaba) (Arxiv) [Qwen-VL] Qwen-VL - A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2023%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen-VL%5D%20Qwen-VL%20-%20A%20Versatile%20Vision-Language%20Model%20for%20Understanding%2C%20Localization%2C%20Text%20Reading%2C%20and%20Beyond.pdf) <br />
* [2024 (Alibaba) (Arxiv) [QWEN2] QWEN2 Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2024%20%28Alibaba%29%20%28Arxiv%29%20%5BQWEN2%5D%20QWEN2%20Technical%20Report.pdf) <br />
* [2024 (Meta) (Arxiv) [Llma3] The Llama 3 Herd of Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2024%20%28Meta%29%20%28Arxiv%29%20%5BLlma3%5D%20The%20Llama%203%20Herd%20of%20Models.pdf) <br />
* [2025 (Alibaba) (Arxiv) [QWEN-2.5] QWEN 2.5 Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQWEN-2.5%5D%20QWEN%202.5%20Technical%20Report.pdf) <br />
* [2025 (Alibaba) (Arxiv) [Qwen2.5-VL] Qwen2.5-VL Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen2.5-VL%5D%20Qwen2.5-VL%20Technical%20Report.pdf) <br />
* [2025 (Alibaba) (Arxiv) [Qwen3 Embedding] Qwen3 Embedding - Advancing Text Embedding and Reranking Through Foundation Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen3%20Embedding%5D%20Qwen3%20Embedding%20-%20Advancing%20Text%20Embedding%20and%20Reranking%20Through%20Foundation%20Models.pdf) <br />
* [2025 (Alibaba) (Arxiv) [Qwen3-VL] Qwen3-VL Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen3-VL%5D%20Qwen3-VL%20Technical%20Report.pdf) <br />
* [2025 (Alibaba) (Arxiv) [Qwen3] Qwen3 Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/3_Alibaba/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen3%5D%20Qwen3%20Technical%20Report.pdf) <br />

#### 4_Meta
* [2020 (Meta) (NIPS) [RAG]  Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2020%20%28Meta%29%20%28NIPS%29%20%5BRAG%5D%20%20Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks.pdf) <br />
* [2023 (Meta) (Arxiv) [LLaMA-2] Llama 2 - Open Foundation and Fine-Tuned ChatModels](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2023%20%28Meta%29%20%28Arxiv%29%20%5BLLaMA-2%5D%20Llama%202%20-%20Open%20Foundation%20and%20Fine-Tuned%20ChatModels.pdf) <br />
* [2023 (Meta) (Arxiv) [LLaMA] LLaMA - Open and Efficient Foundation Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2023%20%28Meta%29%20%28Arxiv%29%20%5BLLaMA%5D%20LLaMA%20-%20Open%20and%20Efficient%20Foundation%20Language%20Models.pdf) <br />
* [2023 (Meta) (Arxiv) [Toolformer] Toolformer - Language Models Can Teach Themselves to Use Tools](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2023%20%28Meta%29%20%28Arxiv%29%20%5BToolformer%5D%20Toolformer%20-%20Language%20Models%20Can%20Teach%20Themselves%20to%20Use%20Tools.pdf) <br />
* [2024 (Arxiv) [TinyLlama] Arxiv TinyLlama - An Open-Source Small Language Model](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2024%20%28Arxiv%29%20%5BTinyLlama%5D%20Arxiv%20TinyLlama%20-%20An%20Open-Source%20Small%20Language%20Model.pdf) <br />
* [2024 (Meta) (Arxiv) [Code Llama] Code Llama - Open Foundation Models for Code](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2024%20%28Meta%29%20%28Arxiv%29%20%5BCode%20Llama%5D%20Code%20Llama%20-%20Open%20Foundation%20Models%20for%20Code.pdf) <br />
* [2024 (Meta) (Arxiv) [LLaMA3] The Llama 3 Herd of Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/4_Meta/2024%20%28Meta%29%20%28Arxiv%29%20%5BLLaMA3%5D%20The%20Llama%203%20Herd%20of%20Models.pdf) <br />

#### 5_Microsoft
* [2023 (Microsoft) (NIPS) [LLaVA] Visual Instruction Tuning](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/00_Organizations/5_Microsoft/2023%20%28Microsoft%29%20%28NIPS%29%20%5BLLaVA%5D%20Visual%20Instruction%20Tuning.pdf) <br />

## 01_AlgorithmOptimization
* [2017 (OpenAI) (Arxiv) [PPO] Proximal Policy Optimization Algorithms](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2017%20%28OpenAI%29%20%28Arxiv%29%20%5BPPO%5D%20Proximal%20Policy%20Optimization%20Algorithms.pdf) <br />
* [2019 (ACL) [Sentence-BERT] Sentence-BERT - Sentence Embeddings using Siamese BERT-Networks](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2019%20%28ACL%29%20%5BSentence-BERT%5D%20Sentence-BERT%20-%20Sentence%20Embeddings%20using%20Siamese%20BERT-Networks.pdf) <br />
* [2019 (Google) (Arxiv) [MQA] Fast Transformer Decoding - One Write-Head is All You Need](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2019%20%28Google%29%20%28Arxiv%29%20%5BMQA%5D%20Fast%20Transformer%20Decoding%20-%20One%20Write-Head%20is%20All%20You%20Need.pdf) <br />
* [2019 (NIPS) [RMSNorm] Root Mean Square Layer Normalization](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2019%20%28NIPS%29%20%5BRMSNorm%5D%20Root%20Mean%20Square%20Layer%20Normalization.pdf) <br />
* [2020 (Microsoft) On Layer Normalization in the Transformer Architecture](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2020%20%28Microsoft%29%20On%20Layer%20Normalization%20in%20the%20Transformer%20Architecture.pdf) <br />
* [2022 (Microsoft) [DeepNorm] DeepNet - Scaling Transformers to 1,000 Layers](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2022%20%28Microsoft%29%20%5BDeepNorm%5D%20DeepNet%20-%20Scaling%20Transformers%20to%201%2C000%20Layers.pdf) <br />
* [2023 (Arxiv) [ROPE] RoFormer - Enhanced Transformer with Rotary Position Embedding](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2023%20%28Arxiv%29%20%5BROPE%5D%20RoFormer%20-%20Enhanced%20Transformer%20with%20Rotary%20Position%20Embedding.pdf) <br />
* [2023 (Google) (Arxiv)[GQA] GQA - Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/2023%20%28Google%29%20%28Arxiv%29%5BGQA%5D%20GQA%20-%20Training%20Generalized%20Multi-Query%20Transformer%20Models%20from%20Multi-Head%20Checkpoints.pdf) <br />

#### Tokenization
* [2016 (ACL) [BPE] Neural Machine Translation of Rare Words with Subword Units](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/01_AlgorithmOptimization/Tokenization/2016%20%28ACL%29%20%5BBPE%5D%20Neural%20Machine%20Translation%20of%20Rare%20Words%20with%20Subword%20Units.pdf) <br />

## 02_SystemOptimization
* [2020 (Microsoft) [ZeRO] ZeRO - Memory Optimizations Toward Training Trillion Parameter Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/02_SystemOptimization/2020%20%28Microsoft%29%20%5BZeRO%5D%20ZeRO%20-%20Memory%20Optimizations%20Toward%20Training%20Trillion%20Parameter%20Models.pdf) <br />
* [2022 (Google) (Arxiv) [KVcache] Efficiently Scaling Transformer Inference](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/02_SystemOptimization/2022%20%28Google%29%20%28Arxiv%29%20%5BKVcache%5D%20Efficiently%20Scaling%20Transformer%20Inference.pdf) <br />
* [2022 (Stanford) (Arxiv)[FlashAttention] FlashAttention - Fast and Memory-Eﬃcient Exact Attention with IO-Awareness](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/02_SystemOptimization/2022%20%28Stanford%29%20%28Arxiv%29%5BFlashAttention%5D%20FlashAttention%20-%20Fast%20and%20Memory-E%EF%AC%83cient%20Exact%20Attention%20with%20IO-Awareness.pdf) <br />
* [2023 (Princeton) (Arxiv) [FlashAttention2] FlashAttention-2 - Faster Attention with Better Parallelism and Work Partitioning](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/02_SystemOptimization/2023%20%28Princeton%29%20%28Arxiv%29%20%5BFlashAttention2%5D%20FlashAttention-2%20-%20Faster%20Attention%20with%20Better%20Parallelism%20and%20Work%20Partitioning.pdf) <br />
* [2024 (Princeton) (Arxiv) [FlashAttention3] FlashAttention-3 - Fast and Accurate Attention with Asynchrony and Low-precision](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/02_SystemOptimization/2024%20%28Princeton%29%20%28Arxiv%29%20%5BFlashAttention3%5D%20FlashAttention-3%20-%20Fast%20and%20Accurate%20Attention%20with%20Asynchrony%20and%20Low-precision.pdf) <br />

## 03_SFT
* [2019 (Google) (ICML) [Adapter] Parameter-Efficient Transfer Learning for NLP](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/03_SFT/2019%20%28Google%29%20%28ICML%29%20%5BAdapter%5D%20Parameter-Efficient%20Transfer%20Learning%20for%20NLP.pdf) <br />
* [2021 (Microsoft) (Arxiv) [LORA] LoRA - Low-Rank Adaptation of Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/03_SFT/2021%20%28Microsoft%29%20%28Arxiv%29%20%5BLORA%5D%20LoRA%20-%20Low-Rank%20Adaptation%20of%20Large%20Language%20Models.pdf) <br />
* [2023 （ACL) [UltraChat] Enhancing Chat Language Models by Scaling High-quality Instructional Conversations](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/03_SFT/2023%20%EF%BC%88ACL%29%20%5BUltraChat%5D%20Enhancing%20Chat%20Language%20Models%20by%20Scaling%20High-quality%20Instructional%20Conversations.pdf) <br />

## 04_RL
* [2022 (OpenAI) (Arxiv) [PPO] Proximal Policy Optimization Algorithms](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/04_RL/2022%20%28OpenAI%29%20%28Arxiv%29%20%5BPPO%5D%20Proximal%20Policy%20Optimization%20Algorithms.pdf) <br />
* [2023 (Stanford) (NIPS) [DPO] Direct Preference Optimization - Your Language Model is Secretly a Reward Model](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/04_RL/2023%20%28Stanford%29%20%28NIPS%29%20%5BDPO%5D%20Direct%20Preference%20Optimization%20-%20Your%20Language%20Model%20is%20Secretly%20a%20Reward%20Model.pdf) <br />
* [2024 (ACL) ORPO - Monolithic Preference Optimization without Reference Model](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/04_RL/2024%20%28ACL%29%20ORPO%20-%20Monolithic%20Preference%20Optimization%20without%20Reference%20Model.pdf) <br />
* [2024 (DeepSeek) (Arxiv) [GRPO] DeepSeekMath - Pushing the Limits of Mathematical Reasoning in Open Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/04_RL/2024%20%28DeepSeek%29%20%28Arxiv%29%20%5BGRPO%5D%20DeepSeekMath%20-%20Pushing%20the%20Limits%20of%20Mathematical%20Reasoning%20in%20Open%20Language%20Models.pdf) <br />
* [2024 (DeepSeek) [GRPO] DeepSeekMath - Pushing the Limits of Mathematical Reasoning in Open Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/04_RL/2024%20%28DeepSeek%29%20%5BGRPO%5D%20DeepSeekMath%20-%20Pushing%20the%20Limits%20of%20Mathematical%20Reasoning%20in%20Open%20Language%20Models.pdf) <br />

## 05_MultiModal
* [2014 (ICML) [VAE] Auto-Encoding Variational Bayes](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2014%20%28ICML%29%20%5BVAE%5D%20Auto-Encoding%20Variational%20Bayes.pdf) <br />
* [2014 (NIPS) [GAN] Generative Adversarial Nets](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2014%20%28NIPS%29%20%5BGAN%5D%20Generative%20Adversarial%20Nets.pdf) <br />
* [2017 (NIPS) [VQ-VAE] Neural Discrete Representation Learning](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2017%20%28NIPS%29%20%5BVQ-VAE%5D%20Neural%20Discrete%20Representation%20Learning.pdf) <br />
* [2020 (Google) (ICLR) [ALBERT] ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2020%20%28Google%29%20%28ICLR%29%20%5BALBERT%5D%20ALBERT%20-%20A%20Lite%20BERT%20for%20Self-supervised%20Learning%20of%20Language%20Representations.pdf) <br />
* [2020 (NIPS) [Diffusion] Denoising Diffusion Probabilistic Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2020%20%28NIPS%29%20%5BDiffusion%5D%20Denoising%20Diffusion%20Probabilistic%20Models.pdf) <br />
* [2021 (Google) (ICLR) [VIT] An Image is Worth 16x16 Words - Transformers for Image Recognition at Scale](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2021%20%28Google%29%20%28ICLR%29%20%5BVIT%5D%20An%20Image%20is%20Worth%2016x16%20Words%20-%20Transformers%20for%20Image%20Recognition%20at%20Scale.pdf) <br />
* [2021 (Google) (ICML) Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2021%20%28Google%29%20%28ICML%29%20Scaling%20Up%20Visual%20and%20Vision-Language%20Representation%20Learning%20With%20Noisy%20Text%20Supervision.pdf) <br />
* [2021 (OpenAI) (ICML) [CLIP] Learning Transferable Visual Models From Natural Language Supervision](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2021%20%28OpenAI%29%20%28ICML%29%20%5BCLIP%5D%20Learning%20Transferable%20Visual%20Models%20From%20Natural%20Language%20Supervision.pdf) <br />
* [2022 (Salesforce) (ICML) [BLIP] BLIP - Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2022%20%28Salesforce%29%20%28ICML%29%20%5BBLIP%5D%20BLIP%20-%20Bootstrapping%20Language-Image%20Pre-training%20for%20Unified%20Vision-Language%20Understanding%20and%20Generation.pdf) <br />
* [2023 (Alibaba) (Arxiv) [Qwen-VL] Qwen-VL - A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2023%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen-VL%5D%20Qwen-VL%20-%20A%20Versatile%20Vision-Language%20Model%20for%20Understanding%2C%20Localization%2C%20Text%20Reading%2C%20and%20Beyond.pdf) <br />
* [2023 (Google (Arxiv) [SIGLIP] Sigmoid Loss for Language Image Pre-Training](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2023%20%28Google%20%28Arxiv%29%20%5BSIGLIP%5D%20Sigmoid%20Loss%20for%20Language%20Image%20Pre-Training.pdf) <br />
* [2023 (Microsoft) (NIPS) [LLaVA] Visual Instruction Tuning](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2023%20%28Microsoft%29%20%28NIPS%29%20%5BLLaVA%5D%20Visual%20Instruction%20Tuning.pdf) <br />
* [2023 (OpenAI) GPT-4V(ision) System Card](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2023%20%28OpenAI%29%20GPT-4V%28ision%29%20System%20Card.pdf) <br />
* [2023 (Salesforce) (ICML) [BLIP-2] BLIP-2 - Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2023%20%28Salesforce%29%20%28ICML%29%20%5BBLIP-2%5D%20BLIP-2%20-%20Bootstrapping%20Language-Image%20Pre-training%20with%20Frozen%20Image%20Encoders%20and%20Large%20Language%20Models.pdf) <br />
* [2024 (Google) (Arxiv) [Gemini] Gemini - A Family of Highly Capable Multimodal Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2024%20%28Google%29%20%28Arxiv%29%20%5BGemini%5D%20Gemini%20-%20A%20Family%20of%20Highly%20Capable%20Multimodal%20Models.pdf) <br />
* [2024 (Google) (Arxiv) [Gemma] Gemma - Open Models Based on Gemini Research and Technology](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2024%20%28Google%29%20%28Arxiv%29%20%5BGemma%5D%20Gemma%20-%20Open%20Models%20Based%20on%20Gemini%20Research%20and%20Technology.pdf) <br />
* [2025 (Alibaba) (Arxiv) [Qwen2.5-VL] Qwen2.5-VL Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen2.5-VL%5D%20Qwen2.5-VL%20Technical%20Report.pdf) <br />
* [2025 (Alibaba) (Arxiv) [Qwen3-VL] Qwen3-VL Technical Report](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2025%20%28Alibaba%29%20%28Arxiv%29%20%5BQwen3-VL%5D%20Qwen3-VL%20Technical%20Report.pdf) <br />
* [2025 (Google (Arxiv) [SIGLIP2] SigLIP 2 - Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/05_MultiModal/2025%20%28Google%20%28Arxiv%29%20%5BSIGLIP2%5D%20SigLIP%202%20-%20Multilingual%20Vision-Language%20Encoders%20with%20Improved%20Semantic%20Understanding%2C%20Localization%2C%20and%20Dense%20Features.pdf) <br />

## 06_MOE
* [2017 (Google) (ICLR) [Sparsely-Gated MOE] Outrageously large neural networks - The sparsely-gated mixture-of-experts layer](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/06_MOE/2017%20%28Google%29%20%28ICLR%29%20%5BSparsely-Gated%20MOE%5D%20Outrageously%20large%20neural%20networks%20-%20The%20sparsely-gated%20mixture-of-experts%20layer.pdf) <br />
* [2018 (Google) (KDD) ** [MMoE] Modeling task relationships in multi-task learning with multi-gate mixture-of-experts](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/06_MOE/2018%20%28Google%29%20%28KDD%29%20%2A%2A%20%5BMMoE%5D%20Modeling%20task%20relationships%20in%20multi-task%20learning%20with%20multi-gate%20mixture-of-experts.pdf) <br />
* [2022 (Google) (JMLR) [SwitchTransfomers] Switch Transformers - Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/06_MOE/2022%20%28Google%29%20%28JMLR%29%20%5BSwitchTransfomers%5D%20Switch%20Transformers%20-%20Scaling%20to%20Trillion%20Parameter%20Models%20with%20Simple%20and%20Efficient%20Sparsity.pdf) <br />
* [2022 (Meta) (EMNLP) Efficient Large Scale Language Modeling with Mixtures of Experts](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/06_MOE/2022%20%28Meta%29%20%28EMNLP%29%20Efficient%20Large%20Scale%20Language%20Modeling%20with%20Mixtures%20of%20Experts.pdf) <br />
* [2024 (Arxiv) [Mixtral] Mixtral of Experts](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/06_MOE/2024%20%28Arxiv%29%20%5BMixtral%5D%20Mixtral%20of%20Experts.pdf) <br />
* [2024 (Google) (ICLR) From Sparse to Soft Mixtures of Experts](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/06_MOE/2024%20%28Google%29%20%28ICLR%29%20From%20Sparse%20to%20Soft%20Mixtures%20of%20Experts.pdf) <br />

## 07_Application
* [2020 (Meta) (NIPS) [RAG]  Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/07_Application/2020%20%28Meta%29%20%28NIPS%29%20%5BRAG%5D%20%20Retrieval-Augmented%20Generation%20for%20Knowledge-Intensive%20NLP%20Tasks.pdf) <br />
* [2023 (Google) (ICLR) [ReAct] ReAct - Synergizing Reasoning and Acting in Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/07_Application/2023%20%28Google%29%20%28ICLR%29%20%5BReAct%5D%20ReAct%20-%20Synergizing%20Reasoning%20and%20Acting%20in%20Language%20Models.pdf) <br />
* [2023 (Meta) (Arxiv) [Toolformer] Toolformer - Language Models Can Teach Themselves to Use Tools](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/07_Application/2023%20%28Meta%29%20%28Arxiv%29%20%5BToolformer%5D%20Toolformer%20-%20Language%20Models%20Can%20Teach%20Themselves%20to%20Use%20Tools.pdf) <br />
* [2024 (Princeton) (ICLR) [Llemma] Llemma - An Open Language Model For Mathematics](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/07_Application/2024%20%28Princeton%29%20%28ICLR%29%20%5BLlemma%5D%20Llemma%20-%20An%20Open%20Language%20Model%20For%20Mathematics.pdf) <br />

## 08_Quant
* [2023 (NIPS) [QLORA] QLORA - Efficient Finetuning of Quantized LLMs](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/08_Quant/2023%20%28NIPS%29%20%5BQLORA%5D%20QLORA%20-%20Efficient%20Finetuning%20of%20Quantized%20LLMs.pdf) <br />

## 09_Evaluation
* [2002 (ACL) BLEU - a Method for Automatic Evaluation of Machine Translation](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2002%20%28ACL%29%20BLEU%20-%20a%20Method%20for%20Automatic%20Evaluation%20of%20Machine%20Translation.pdf) <br />
* [2004 (ACL) ROUGE - A Package for Automatic Evaluation of Summaries](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2004%20%28ACL%29%20ROUGE%20-%20A%20Package%20for%20Automatic%20Evaluation%20of%20Summaries.pdf) <br />
* [2019 (ICLR) (DeepMind) GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2019%20%28ICLR%29%20%28DeepMind%29%20GLUE%20-%20A%20Multi-Task%20Benchmark%20and%20Analysis%20Platform%20for%20Natural%20Language%20Understanding.pdf) <br />
* [2021 (Arxiv) (OpenAI) Evaluating Large Language Models Trained on Code](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2021%20%28Arxiv%29%20%28OpenAI%29%20Evaluating%20Large%20Language%20Models%20Trained%20on%20Code.pdf) <br />
* [2022 (ACL) (OpenAI) TruthfulQA - Measuring How Models Mimic Human Falsehoods](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2022%20%28ACL%29%20%28OpenAI%29%20TruthfulQA%20-%20Measuring%20How%20Models%20Mimic%20Human%20Falsehoods.pdf) <br />
* [2023 (NIPS) Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2023%20%28NIPS%29%20Judging%20LLM-as-a-Judge%20with%20MT-Bench%20and%20Chatbot%20Arena.pdf) <br />
* [2024 (Arxiv) Chatbot Arena - An Open Platform for Evaluating LLMs by Human Preference](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/09_Evaluation/2024%20%28Arxiv%29%20Chatbot%20Arena%20-%20An%20Open%20Platform%20for%20Evaluating%20LLMs%20by%20Human%20Preference.pdf) <br />

## 10_Resources
* [LLM book FDU zh](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/10_Resources/LLM%20book%20FDU%20zh.pdf) <br />
* [LLM book RUC zh](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/10_Resources/LLM%20book%20RUC%20zh.pdf) <br />
* [resources](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/10_Resources/resources.txt) <br />

## 11_Survey
* [(2021) (Arxiv) Pre-train, Prompt, and Predict - A Systematic Survey of Prompting Methods in Natural Language Processing](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/11_Survey/%282021%29%20%28Arxiv%29%20Pre-train%2C%20Prompt%2C%20and%20Predict%20-%20A%20Systematic%20Survey%20of%20Prompting%20Methods%20in%20Natural%20Language%20Processing.pdf) <br />
* [(2024) (Arxiv) A Survey of Large Language Models for Recommendation](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/11_Survey/%282024%29%20%28Arxiv%29%20A%20Survey%20of%20Large%20Language%20Models%20for%20Recommendation.pdf) <br />
* [(2024) (Arxiv) A Survey of Multimodal Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/11_Survey/%282024%29%20%28Arxiv%29%20A%20Survey%20of%20Multimodal%20Large%20Language%20Models.pdf) <br />
* [(2024) (Arxiv) Large Language Models for Information Retrieval - A Survey](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/11_Survey/%282024%29%20%28Arxiv%29%20Large%20Language%20Models%20for%20Information%20Retrieval%20-%20A%20Survey.pdf) <br />
* [(2025) (Arxiv) A Survey of Large Language Models](https://github.com/guyulongcs/Awesome-LLM-papers/blob/master/11_Survey/%282025%29%20%28Arxiv%29%20A%20Survey%20of%20Large%20Language%20Models.pdf) <br />
